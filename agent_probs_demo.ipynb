{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c8818c2-5502-4045-8b8d-1a46e56017e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cp/v9x7jh4x7f9b1v1bp5tjcjlm0000gn/T/ipykernel_78071/4158054529.py:65: DeprecationWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if is_datetime64tz_dtype(df['series_datetime']):\n",
      "/var/folders/cp/v9x7jh4x7f9b1v1bp5tjcjlm0000gn/T/ipykernel_78071/4158054529.py:65: DeprecationWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if is_datetime64tz_dtype(df['series_datetime']):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 6230 rows across all matches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 7/7 [12:24<00:00, 106.39s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Deduped output saved to: /Users/samharwood/Downloads/agent_probs_demo.csv\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pandas.api.types import is_datetime64tz_dtype\n",
    "\n",
    "FEATURES_PATH = \"data/vlr_features_demo.csv\"\n",
    "MAP_PRED_PATH = \"data/vlr_map_preds_demo.csv\"\n",
    "OUTPUT_PATH   = \"data/vlr_agent_probs_demo.csv\"\n",
    "\n",
    "#hyperparams, placceholder values used\n",
    "PATCH_LAM = 0.5 #patch decay rate\n",
    "REC_LAM   = 0.5 #time-based recency decay\n",
    "CORE_LAM  = 0.5 #roster continuity decay\n",
    "MOV_LAM   = 0.5 #margin of victory weight\n",
    "PATCH_WINDOW = 1 #number of patch versions to consider\n",
    "REQ_SAMPLE_RATIO = 5 / 7 \n",
    "BATCH_SIZE = 1000 #processing batch size\n",
    "PLAYER_WEIGHT = 0.6 \n",
    "TEAM_WEIGHT   = 0.2\n",
    "META_WEIGHT   = 0.2\n",
    "PLAYER_NT_HISTORICAL_WEIGHT = 0.5 #weight for non-team-specific player history\n",
    "MIN_CUTOFF = 0.05 #minimum probability threshold for redistribution\n",
    "\n",
    "#role caps with placeholders\n",
    "ROLE_HARD_CAP = {\"Duelist\": 1.5, \"Initiator\": 1.7, \"Controller\": 1.5, \"Sentinel\": 1.9}\n",
    "AGENT_ROLE = {\n",
    "    'Jett':'Duelist','Reyna':'Duelist','Phoenix':'Duelist','Yoru':'Duelist','Neon':'Duelist',\n",
    "    'Raze':'Duelist','Iso':'Duelist','Waylay':'Duelist',\n",
    "    'Sova':'Initiator','Skye':'Initiator','Breach':'Initiator','Kayo':'Initiator',\n",
    "    'Fade':'Initiator','Gekko':'Initiator','Tejo':'Initiator',\n",
    "    'Brimstone':'Controller','Omen':'Controller','Viper':'Controller','Astra':'Controller',\n",
    "    'Harbor':'Controller','Clove':'Controller',\n",
    "    'Sage':'Sentinel','Killjoy':'Sentinel','Cypher':'Sentinel','Chamber':'Sentinel',\n",
    "    'Deadlock':'Sentinel','Vyse':'Sentinel'\n",
    "}\n",
    "\n",
    "#standardize team names; trim whitespace, collapse known aliases\n",
    "def canonicalize_team(name: str) -> str:\n",
    "    if not isinstance(name, str): return str(name)\n",
    "    s = name.strip()\n",
    "    return \"NRG\" if s.lower() == \"nrg esports\" else s\n",
    "\n",
    "#compute roster stability weight from days since columns using exponential decay (higher gap=lower confidence)\n",
    "def roster_confidence(r, lam):\n",
    "    return (0.2 * (1 - np.exp(-lam * r['days_since_3_of_5'])) +\n",
    "            0.3 * (1 - np.exp(-lam * r['days_since_4_of_5'])) +\n",
    "            0.5 * (1 - np.exp(-lam * r['days_since_5_of_5'])))\n",
    "\n",
    "#bucket numeric valeus to nearest lowest hundred; invalids coerced to zero. more intuitive patch handling\n",
    "def to_bucket(s):\n",
    "    s = pd.to_numeric(s, errors='coerce').fillna(0).astype(int)\n",
    "    return (s // 100) * 100\n",
    "\n",
    "features = pd.read_csv(FEATURES_PATH, parse_dates=['series_datetime'])\n",
    "\n",
    "map_preds = pd.read_csv(MAP_PRED_PATH, parse_dates=['series_datetime_canonical'])\n",
    "map_preds['series_datetime'] = map_preds['series_datetime_canonical']\n",
    "\n",
    "for df in (features, map_preds):\n",
    "    #ensure parsed then strip tz if present\n",
    "    df['series_datetime'] = pd.to_datetime(df['series_datetime'], errors='coerce')\n",
    "    if is_datetime64tz_dtype(df['series_datetime']):\n",
    "        df['series_datetime'] = df['series_datetime'].dt.tz_convert(None)\n",
    "\n",
    "#clean features\n",
    "features['patch_index'] = pd.to_numeric(features['patch_index'], errors='coerce').fillna(0).astype(int)\n",
    "features['days_since_last_match'] = pd.to_numeric(features['days_since_last_match'], errors='coerce').fillna(999)\n",
    "for k in (3,4,5):\n",
    "    features[f'days_since_{k}_of_5'] = pd.to_numeric(features.get(f'days_since_{k}_of_5', 999), errors='coerce').fillna(999).astype(int)\n",
    "features['margin_of_victory'] = pd.to_numeric(features.get('margin_of_victory', 0), errors='coerce').fillna(0)\n",
    "features['player_team_full'] = features['player_team_full'].astype(str).map(canonicalize_team)\n",
    "features['map_name'] = features['map_name'].astype(str).str.lower()\n",
    "features['role'] = features['player_agent'].map(AGENT_ROLE)\n",
    "features = features.drop_duplicates(subset=['series_datetime','player_handle','map_name','player_agent'])\n",
    "features['patch_bucket'] = to_bucket(features['patch_index'])\n",
    "\n",
    "#build per-match team rosters; unique, sorted player handles per (team, series_datetime)\n",
    "team_match_rosters = (\n",
    "    features\n",
    "      .groupby(['player_team_full','series_datetime'], as_index=False)['player_handle']\n",
    "      .apply(lambda s: sorted(set(s)))\n",
    "      .rename(columns={'player_handle':'roster_handles'})\n",
    ")\n",
    "\n",
    "#cache recent unique players overall per team to use as fallback\n",
    "features_sorted = features.sort_values(['player_team_full','series_datetime'], ascending=[True, False])\n",
    "\n",
    "#derive patch_bucket from patch_index, patch_major\n",
    "if 'patch_index' in map_preds.columns:\n",
    "    map_preds['patch_bucket'] = to_bucket(map_preds['patch_index'])\n",
    "elif 'patch_major' in map_preds.columns:\n",
    "    map_preds['patch_bucket'] = to_bucket(map_preds['patch_major'] * 100)\n",
    "else:\n",
    "    raise KeyError(\"map_preds must have either 'patch_index' or 'patch_major'\")\n",
    "\n",
    "#canonicalize team names\n",
    "for col in ['team1_name','team2_name']:\n",
    "    if col in map_preds.columns:\n",
    "        map_preds[col] = map_preds[col].astype(str).map(canonicalize_team)\n",
    "\n",
    "#lowercase map names for consistent joins\n",
    "map_preds['map_name'] = map_preds['map_name'].astype(str).str.lower()\n",
    "\n",
    "#indexes\n",
    "all_agents = sorted(features['player_agent'].unique()) \n",
    "features_team_map = features.set_index(['player_handle','map_name','player_team_full']).sort_index()\n",
    "features_by_player_map = features.set_index(['player_handle','map_name']).sort_index()\n",
    "player_map_cache = {}\n",
    "\n",
    "#compute agent pick probabilities weighted by patch recency, match recency, roster confidence, MOV\n",
    "def weighted_probs(df, patch_bucket):\n",
    "    if df.empty:\n",
    "        return pd.Series(0.0, index=all_agents)\n",
    "    dp = np.maximum(0, patch_bucket - df['patch_bucket'])\n",
    "    w_patch = 1.0 / (1.0 + PATCH_LAM * np.log1p(dp))\n",
    "    w_time  = np.exp(-REC_LAM * df['days_since_last_match'])\n",
    "    conf    = roster_confidence(df, CORE_LAM)\n",
    "    mov     = 1 + MOV_LAM * df['margin_of_victory']\n",
    "    wt      = w_patch * w_time * conf * mov\n",
    "    sums    = pd.Series(wt).groupby(df['player_agent']).sum()\n",
    "    out     = sums.reindex(all_agents, fill_value=0.0)\n",
    "    return out/out.sum() if out.sum()>0 else pd.Series(0.0, index=all_agents)\n",
    "\n",
    "#compute team and meta level agent pick probabilities on a given map prior to dt, weighted by patch/time decay\n",
    "def get_team_meta_probs(team_name, mp, patch_bucket, dt):\n",
    "    team_hist = features[(features['map_name']==mp) & (features['player_team_full']==team_name) & (features['series_datetime']<dt)].copy()\n",
    "    meta      = features[(features['map_name']==mp) & (features['series_datetime']<dt) & (features['player_team_full']!=team_name)].copy()\n",
    "    if PATCH_WINDOW:\n",
    "        team_hist = team_hist[team_hist['patch_bucket'] >= (patch_bucket - PATCH_WINDOW*100)]\n",
    "        meta      = meta[meta['patch_bucket'] >= (patch_bucket - PATCH_WINDOW*100)]\n",
    "    team_probs = weighted_probs(team_hist, patch_bucket)\n",
    "    meta_probs = weighted_probs(meta, patch_bucket)\n",
    "    return team_probs.reindex(all_agents, fill_value=0.0), meta_probs.reindex(all_agents, fill_value=0.0)\n",
    "\n",
    "#compute player-level agent pick probabilities on a given map before dt, preferring team-specific history and falling back to global player history with caching\n",
    "def get_player_probs(player, mp, team, patch_bucket, dt):\n",
    "    try:\n",
    "        hist = features_team_map.loc[(player, mp, team)]\n",
    "        hist = hist[hist['series_datetime'] < dt]\n",
    "        if PATCH_WINDOW:\n",
    "            hist = hist[hist['patch_bucket'] >= (patch_bucket - PATCH_WINDOW*100)]\n",
    "        if not hist.empty:\n",
    "            return weighted_probs(hist, patch_bucket)\n",
    "    except KeyError:\n",
    "        pass\n",
    "    key = (player, mp, patch_bucket, dt)\n",
    "    if key in player_map_cache:\n",
    "        return player_map_cache[key]\n",
    "    try:\n",
    "        hist = features_by_player_map.loc[(player, mp)]\n",
    "        hist = hist[hist['series_datetime'] < dt]\n",
    "        if PATCH_WINDOW:\n",
    "            hist = hist[hist['patch_bucket'] >= (patch_bucket - PATCH_WINDOW*100)]\n",
    "        p = weighted_probs(hist, patch_bucket) if not hist.empty else pd.Series(0.0, index=all_agents)\n",
    "    except KeyError:\n",
    "        p = pd.Series(0.0, index=all_agents)\n",
    "    res = p * PLAYER_NT_HISTORICAL_WEIGHT\n",
    "    player_map_cache[key] = res\n",
    "    return res\n",
    "\n",
    "#get most recent active 5-player roster for a team as of a specific datetime with edge case handling\n",
    "def get_active_roster(team: str, dt: pd.Timestamp, features_df: pd.DataFrame) -> list[str]:\n",
    "\n",
    "    team = canonicalize_team(team)\n",
    "    # Latest prior team match roster\n",
    "    prior = team_match_rosters[(team_match_rosters['player_team_full'] == team) &\n",
    "                               (team_match_rosters['series_datetime'] < dt)]\n",
    "    if not prior.empty:\n",
    "        latest_idx = prior['series_datetime'].idxmax()\n",
    "        last_roster = list(prior.loc[latest_idx, 'roster_handles'])\n",
    "    else:\n",
    "        last_roster = []\n",
    "\n",
    "    #if roster has 5 players, return it without further modification\n",
    "    if len(last_roster) >= 5:\n",
    "        return last_roster[:5]\n",
    "\n",
    "    #fallback: fill with most recent unique players overall (excluding those already selected)\n",
    "    team_hist = features_sorted[(features_sorted['player_team_full'] == team) &\n",
    "                                (features_sorted['series_datetime'] < dt)]\n",
    "    recent_uniques = team_hist.drop_duplicates(subset=['player_handle'])['player_handle'].tolist()\n",
    "    #merge, preserving last_roster order first\n",
    "    seen = set(last_roster)\n",
    "    for ph in recent_uniques:\n",
    "        if ph not in seen:\n",
    "            last_roster.append(ph)\n",
    "            seen.add(ph)\n",
    "        if len(last_roster) >= 5:\n",
    "            break\n",
    "\n",
    "    return last_roster[:5]\n",
    "\n",
    "#main loop; split map_preds into record batches for processing, each batch contains up to specified BATCH_SIZE rows\n",
    "rows = map_preds.to_dict(orient='records')\n",
    "batches = [rows[i:i+BATCH_SIZE] for i in range(0, len(rows), BATCH_SIZE)]\n",
    "results = []\n",
    "print(f\"Processing {len(rows)} rows across all matches...\")\n",
    "\n",
    "#generates player-agent prob records for team/map combos by combining player, team, and meta distributions weighted by map prediction probs\n",
    "for batch in tqdm(batches, desc=\"Processing\", unit=\"batch\"):\n",
    "    for row in batch:\n",
    "        match_id     = row['MatchID']\n",
    "        dt           = row['series_datetime'] \n",
    "        patch_bucket = row['patch_bucket']\n",
    "        mp           = row['map_name']\n",
    "        prob         = row['map_predicted_prob']\n",
    "\n",
    "        # enforce last match's roster (up to 5), with fallback to recent uniques\n",
    "        team_names = []\n",
    "        if 'team1_name' in row and 'team2_name' in row:\n",
    "            team_names = [row['team1_name'], row['team2_name']]\n",
    "        else:\n",
    "            team_names = list({row.get('team_left_full'), row.get('team_right_full')})\n",
    "\n",
    "        for team in team_names:\n",
    "            team = canonicalize_team(team)\n",
    "            players = get_active_roster(team, dt, features)\n",
    "            if len(players) > 5:\n",
    "                players = players[:5]\n",
    "\n",
    "            team_probs, meta_probs = get_team_meta_probs(team, mp, patch_bucket, dt)\n",
    "\n",
    "            for player in players:\n",
    "                p_probs = get_player_probs(player, mp, team, patch_bucket, dt)\n",
    "                final = PLAYER_WEIGHT*p_probs + TEAM_WEIGHT*team_probs + META_WEIGHT*meta_probs\n",
    "                if final.sum() > 0:\n",
    "                    final = final / final.sum()\n",
    "                for agent, ap in final.items():\n",
    "                    results.append({\n",
    "                        'MatchID': match_id,\n",
    "                        'series_datetime': dt,\n",
    "                        'player_handle': player,\n",
    "                        'player_agent': agent,\n",
    "                        'map_name': mp,\n",
    "                        'player_team_full': team,\n",
    "                        'role': AGENT_ROLE.get(agent, 'NA'),\n",
    "                        'per_map_prob': ap,\n",
    "                        'raw_prob': ap * prob,\n",
    "                        'map_prob': prob\n",
    "                    })\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "#series-level normalization\n",
    "map_counts = map_preds.groupby('MatchID')['map_was_played'].sum()\n",
    "df['num_maps'] = df['MatchID'].map(map_counts)\n",
    "df['normalized_prob'] = df.groupby(['MatchID','player_handle'])['raw_prob'] \\\n",
    "                          .transform(lambda x: (x/x.sum()) if x.sum()>0 else x) * df['num_maps']\n",
    "\n",
    "#role caps applied at team level\n",
    "role_totals = df.groupby(['MatchID','map_name','player_team_full','role'], observed=True)['normalized_prob'] \\\n",
    "                .sum().reset_index().rename(columns={'normalized_prob':'expected_role_count'})\n",
    "df = df.merge(role_totals, on=['MatchID','map_name','player_team_full','role'], how='left')\n",
    "cap = df['role'].map(ROLE_HARD_CAP).fillna(np.inf)\n",
    "scale = np.where(df['expected_role_count'] > cap, cap/df['expected_role_count'], 1.0)\n",
    "df['adjusted_prob'] = df['normalized_prob'] * scale\n",
    "\n",
    "#re-normalization on matchid, player_handle, map_name after team scaling\n",
    "df['adjusted_prob'] = df.groupby(['MatchID','player_handle','map_name'])['adjusted_prob'] \\\n",
    "                        .transform(lambda x: (x/x.sum()) if x.sum()>0 else x)\n",
    "\n",
    "#min_cutoff redistribution; agents with extremely low probabilities like generated from meta dist are redistributed to higher probability agents that make more sense at a player level\n",
    "low  = df[df['adjusted_prob'] < MIN_CUTOFF].copy()\n",
    "keep = df[df['adjusted_prob'] >= MIN_CUTOFF].copy()\n",
    "rem  = keep.groupby(['MatchID','player_handle','map_name'])['adjusted_prob'].sum().reset_index().rename(columns={'adjusted_prob':'remaining'})\n",
    "add  = low.groupby(['MatchID','player_handle','map_name'])['adjusted_prob'].sum().reset_index().rename(columns={'adjusted_prob':'to_redist'})\n",
    "keep = keep.merge(rem, on=['MatchID','player_handle','map_name'], how='left') \\\n",
    "           .merge(add, on=['MatchID','player_handle','map_name'], how='left').fillna(0.0)\n",
    "mask = keep['remaining'] > 0\n",
    "keep.loc[mask, 'adjusted_prob'] = keep.loc[mask, 'adjusted_prob'] * (1 + keep.loc[mask, 'to_redist'] / keep.loc[mask, 'remaining'])\n",
    "keep = keep.drop(columns=['remaining','to_redist'])\n",
    "keep['adjusted_prob'] = keep.groupby(['MatchID','player_handle','map_name'])['adjusted_prob'] \\\n",
    "                            .transform(lambda x: (x/x.sum()) if x.sum()>0 else x)\n",
    "\n",
    "#convert per-map probabilities into final series (match) level probabilities; normalize\n",
    "keep['raw_series_prob'] = keep['adjusted_prob'] * keep['map_prob']\n",
    "keep['final_series_prob'] = keep.groupby(['MatchID','player_handle'])['raw_series_prob'] \\\n",
    "                               .transform(lambda x: (x/x.sum()) if x.sum()>0 else x) * keep['num_maps']\n",
    "\n",
    "agg = (keep.groupby(['MatchID','player_handle','player_agent','map_name'], as_index=False)\n",
    "          .agg({'adjusted_prob':'sum','map_prob':'sum','per_map_prob':'sum','final_series_prob':'sum'}))\n",
    "meta = keep[['MatchID','player_handle','player_agent','map_name','series_datetime','player_team_full','role']] \\\n",
    "       .drop_duplicates()\n",
    "grouped = agg.merge(meta, on=['MatchID','player_handle','player_agent','map_name'], how='left')\n",
    "\n",
    "grouped = grouped[['MatchID','series_datetime','player_handle','player_agent','map_name',\n",
    "                   'player_team_full','role','adjusted_prob','map_prob','per_map_prob','final_series_prob']]\n",
    "\n",
    "grouped.to_csv(OUTPUT_PATH, index=False)\n",
    "print(f\"Done. Deduped output saved to: {OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da11974-1393-4575-8ff9-1589608554d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (juicer)",
   "language": "python",
   "name": "juicer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
