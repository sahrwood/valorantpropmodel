{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0faa7beb-64aa-4c23-9c5d-2d45ca320009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows before 7-map gate: 6162\n",
      "Rows after 7-map gate: 3496\n",
      "PoolID gaps check: [1, 2]\n",
      "NaN patch_index rows: 186\n",
      "Final output: 3264 records saved → /Users/samharwood/Downloads/vlr_patchpool_demo.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "ROUNDSTATS_CSV   = \"data/vlr_roundstats_demo.csv\"\n",
    "PLAYER_STATS_CSV = \"data/vlr_playerstats_demo.csv\"\n",
    "MATCHDATA_CSV    = \"data/vlr_matchstats_demo.csv\"\n",
    "OUT_PATH         = \"output/vlr_patchpool_demo.csv\"\n",
    "\n",
    "\n",
    "roundstats = pd.read_csv(ROUNDSTATS_CSV)\n",
    "player_stats = pd.read_csv(PLAYER_STATS_CSV)\n",
    "df_matchdata = pd.read_csv(MATCHDATA_CSV)\n",
    "\n",
    "#converts wide roundstats (both teams per row) into long team-level rows, enabling clean Round_Diff aggregation later\n",
    "team_rounds = []\n",
    "for _, row in roundstats.iterrows():\n",
    "    team_rounds.extend([\n",
    "        {'game_id': row['game_id'], 'team_num': 1, 'result': row['team1_result']},\n",
    "        {'game_id': row['game_id'], 'team_num': 2, 'result': row['team2_result']},\n",
    "    ])\n",
    "\n",
    "#create team-level round df; drop invalid ids, add binary win indicator\n",
    "round_df = pd.DataFrame(team_rounds)\n",
    "round_df['is_win'] = (round_df['result'] == 'won').astype(int)\n",
    "\n",
    "#group by game_id/team_num to count total rounds/wins, then calculate per-team round diff\n",
    "total_rounds = round_df.groupby(['game_id', 'team_num']).size().reset_index(name='total_rounds')\n",
    "wins_by_team = round_df.groupby(['game_id', 'team_num'])['is_win'].sum().reset_index(name='wins')\n",
    "round_diff_lookup = pd.merge(total_rounds, wins_by_team, on=['game_id', 'team_num'])\n",
    "round_diff_lookup['Round_Diff'] = 2 * round_diff_lookup['wins'] - round_diff_lookup['total_rounds']\n",
    "\n",
    "#nested lookup to retrieve individual team round diffs at a game_id level\n",
    "rd_lookup = {}\n",
    "for _, r in round_diff_lookup.iterrows():\n",
    "    rd_lookup.setdefault(r['game_id'], {})[r['team_num']] = r['Round_Diff']\n",
    "\n",
    "#converts match_datetime into datetime64 objects; error handling/force to UTC\n",
    "df_matchdata['match_datetime'] = pd.to_datetime(df_matchdata['match_datetime'], utc=True, errors='coerce')\n",
    "df_matchdata['game_id'] = pd.to_numeric(df_matchdata['game_id'], errors='coerce').astype('Int64')\n",
    "\n",
    "pb_cols = [f'picks_and_bans.{i}' for i in range(1, 8)]\n",
    "for i in range(1, 8):\n",
    "    df_matchdata[f'picks_and_bans.{i}'] = None\n",
    "\n",
    "#picks and bans parsing, stripping brackets/quotes, splitting commas, returning 7 elements that comprise a match-level pick and ban process\n",
    "def parse_picks_bans(pb_string):\n",
    "    if pd.isna(pb_string):\n",
    "        return [None] * 7\n",
    "    s = str(pb_string).strip('{}').replace(\"'\", \"\")\n",
    "    items = [item.strip() for item in s.split(',')]\n",
    "    return (items + [None] * 7)[:7]\n",
    "\n",
    "#applies parsing, expands picks and bans into columns\n",
    "pb_parsed = df_matchdata['picks_and_bans'].apply(parse_picks_bans).tolist()\n",
    "for i, col in enumerate(pb_cols):\n",
    "    df_matchdata[col] = [row[i] for row in pb_parsed]\n",
    "\n",
    "#define series grouping keys, build matchid counter\n",
    "series_groups = df_matchdata.groupby([\n",
    "    'match_datetime', 'team1_name', 'team2_name', 'competition_name', 'match_best_of'\n",
    "])\n",
    "\n",
    "series_records = []\n",
    "match_id_counter = 1\n",
    "\n",
    "#for each series group: order maps by game_id, collect map ids, attach picks and bans, and subsequently create matchid\n",
    "for group_key, group_df in series_groups:\n",
    "    unique_games = sorted(group_df['game_id'].dropna().astype(int).unique())\n",
    "    \n",
    "    if len(unique_games) > 1:\n",
    "        start = unique_games[0]\n",
    "        expected = list(range(start, start + len(unique_games)))\n",
    "        if unique_games != expected:\n",
    "            continue\n",
    "    \n",
    "    first_game = group_df.iloc[0]\n",
    "    patch_mode = group_df['match_patch'].mode()\n",
    "    match_patch = patch_mode.iat[0] if not patch_mode.empty else first_game['match_patch']\n",
    "    \n",
    "    map_ids = {}\n",
    "    for i in range(1, 6):\n",
    "        map_ids[f'Map{i}_ID'] = unique_games[i-1] if i <= len(unique_games) else None\n",
    "    \n",
    "    pb_data = {col: first_game[col] for col in pb_cols}\n",
    "    \n",
    "    team1, team2 = group_key[1], group_key[2]\n",
    "    \n",
    "    for team_name in [team1, team2]:\n",
    "        team_num = 1 if team_name == team1 else 2\n",
    "        \n",
    "        map_rds = {}\n",
    "        for i in range(1, 6):\n",
    "            mid = map_ids[f'Map{i}_ID']\n",
    "            if pd.notna(mid) and int(mid) in rd_lookup and team_num in rd_lookup[int(mid)]:\n",
    "                map_rds[f'Map{i}_RD'] = rd_lookup[int(mid)][team_num]\n",
    "            else:\n",
    "                map_rds[f'Map{i}_RD'] = pd.NA\n",
    "        \n",
    "        sum_map_rd = sum(v for v in map_rds.values() if pd.notna(v))\n",
    "        \n",
    "        series_records.append({\n",
    "            'MatchID': match_id_counter,\n",
    "            'team': team_name,\n",
    "            'team1_name': team1,\n",
    "            'team2_name': team2,\n",
    "            'series_datetime': group_key[0],\n",
    "            'competition_name': group_key[3],\n",
    "            'match_best_of': group_key[4],\n",
    "            'match_patch': match_patch,\n",
    "            'SeriesRoundDiff': sum_map_rd,\n",
    "            **map_ids,\n",
    "            **map_rds,\n",
    "            **pb_data\n",
    "        })\n",
    "    \n",
    "    match_id_counter += 1\n",
    "\n",
    "df_series = pd.DataFrame(series_records)\n",
    "\n",
    "for i in range(1, 6):\n",
    "    df_series[f\"Map{i}_ID\"] = pd.to_numeric(df_series[f\"Map{i}_ID\"], errors='coerce')\n",
    "\n",
    "df_series['team'] = df_series['team'].astype(str).str.strip()\n",
    "\n",
    "player_stats['player_team_abbrev'] = player_stats['player_team'].str.extract(r'^([^\\(]+)')[0].str.strip()\n",
    "player_stats['player_team_full'] = player_stats['player_team'].str.extract(r'\\(([^)]+)\\)')[0].str.strip()\n",
    "\n",
    "mapping = player_stats[['player_team_abbrev', 'player_team_full']].drop_duplicates()\n",
    "full_to_all_abbrevs = mapping.groupby('player_team_full')['player_team_abbrev'].apply(set).to_dict()\n",
    "\n",
    "def extract_pb_per_team(df, team_num):\n",
    "    records = []\n",
    "    team_col = f'team{team_num}_name'\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        known = full_to_all_abbrevs.get(row[team_col], set())\n",
    "        for col in pb_cols:\n",
    "            parts = str(row.get(col, '')).split(maxsplit=2)\n",
    "            if len(parts) == 3:\n",
    "                abbr, action, map_name = parts\n",
    "                if abbr in known:\n",
    "                    records.append({'idx': idx, 'team_num': team_num, 'action': action, 'map_name': map_name})\n",
    "    \n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "def pivot_pb(df, team_num, action):\n",
    "    if len(df) == 0:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    sub = df[(df['team_num'] == team_num) & (df['action'] == action)].copy()\n",
    "    if len(sub) == 0:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    sub['n'] = sub.groupby('idx').cumcount() + 1\n",
    "    wide = sub.pivot(index='idx', columns='n', values='map_name')\n",
    "    wide.columns = [f'team{team_num}_{action}_{i}' for i in wide.columns]\n",
    "    return wide\n",
    "\n",
    "pb_team1 = extract_pb_per_team(df_series, 1)\n",
    "pb_team2 = extract_pb_per_team(df_series, 2)\n",
    "\n",
    "p1_pick = pivot_pb(pb_team1, 1, 'pick')\n",
    "p2_pick = pivot_pb(pb_team2, 2, 'pick')\n",
    "p1_ban = pivot_pb(pb_team1, 1, 'ban')\n",
    "p2_ban = pivot_pb(pb_team2, 2, 'ban')\n",
    "\n",
    "for name, pivot_table in [('p1_pick', p1_pick), ('p1_ban', p1_ban), ('p2_pick', p2_pick), ('p2_ban', p2_ban)]:\n",
    "    if len(pivot_table) > 0:\n",
    "        overlap = set(pivot_table.columns).intersection(set(df_series.columns))\n",
    "        if overlap:\n",
    "            print(f\"Dropping overlapping columns from {name}: {overlap}\")\n",
    "            df_series = df_series.drop(columns=list(overlap))\n",
    "        df_series = df_series.join(pivot_table, how='left')\n",
    "\n",
    "have = {'team1_pick_1','team2_pick_1'}.intersection(df_series.columns)\n",
    "if have:\n",
    "    df_series = df_series.groupby('Map1_ID').filter(\n",
    "        lambda g: not all(g[c].isna().all() for c in have)\n",
    "    )\n",
    "\n",
    "df_final = pd.merge(\n",
    "    df_series,\n",
    "    player_stats[['game_id', 'player_team_abbrev', 'player_team_full']],\n",
    "    left_on=['Map1_ID', 'team'],\n",
    "    right_on=['game_id', 'player_team_full'],\n",
    "    how='left',\n",
    "    suffixes=('', '_ps')\n",
    ").drop(columns=['game_id'])\n",
    "\n",
    "df_final = df_final.drop_duplicates(subset=['MatchID', 'team'])\n",
    "\n",
    "mapname_lkup = df_matchdata.set_index('game_id')['map_name'].to_dict()\n",
    "for i in range(1, 6):\n",
    "    df_final[f\"Map{i}_name\"] = df_final[f\"Map{i}_ID\"].map(mapname_lkup)\n",
    "\n",
    "series_dt = df_matchdata[['game_id', 'match_datetime']].dropna().drop_duplicates().rename(\n",
    "    columns={'game_id': 'series_id', 'match_datetime': 'series_datetime'}\n",
    ")\n",
    "\n",
    "series_roster = (\n",
    "    player_stats.rename(columns={'game_id': 'series_id', 'player_team_full': 'team_full'})\n",
    "    .groupby(['series_id', 'team_full'])['player_handle']\n",
    "    .apply(lambda lst: sorted(set(lst)))\n",
    "    .reset_index()\n",
    "    .merge(series_dt, on='series_id', how='left')\n",
    ")\n",
    "\n",
    "def _canon_handle(s):\n",
    "    return str(s).strip().lower()\n",
    "\n",
    "def _to_utc(dt):\n",
    "    if pd.isna(dt):\n",
    "        return None\n",
    "    if getattr(dt, \"tzinfo\", None) is None:\n",
    "        return dt.replace(tzinfo=timezone.utc)\n",
    "    return dt.astimezone(timezone.utc)\n",
    "\n",
    "ROSTER_CONTINUITY_RULES = [\n",
    "    {\"from_brand\": \"Envy\", \"to_brand\": \"OpTic Gaming\", \"effective_ts\": datetime(2022, 2, 20, 21, 0, 0, tzinfo=timezone.utc)},\n",
    "    {\"from_brand\": \"Rankers\", \"to_brand\": \"Envy\", \"effective_ts\": datetime(2025, 3, 10, 20, 0, 0, tzinfo=timezone.utc)},\n",
    "]\n",
    "\n",
    "BRAND_RESETS = {\n",
    "    \"Version1\": [datetime(2023, 1, 10, 23, 0, 0, tzinfo=timezone.utc)],\n",
    "}\n",
    "\n",
    "forward_map = {}\n",
    "reverse_map = {}\n",
    "\n",
    "for r in ROSTER_CONTINUITY_RULES:\n",
    "    f = r[\"from_brand\"].strip()\n",
    "    t = r[\"to_brand\"].strip()\n",
    "    ts = r[\"effective_ts\"]\n",
    "    forward_map.setdefault(f, []).append((t, ts))\n",
    "    reverse_map.setdefault(t, []).append((f, ts))\n",
    "\n",
    "def _era_for(brand: str, when_utc):\n",
    "    if when_utc is None:\n",
    "        return 1\n",
    "    resets = BRAND_RESETS.get((brand or \"\").strip(), [])\n",
    "    if not resets:\n",
    "        return 1\n",
    "    cnt = sum(1 for ts in resets if when_utc >= ts)\n",
    "    return 1 + cnt\n",
    "\n",
    "def continuity_key_for(brand: str, when):\n",
    "    brand = (brand or \"\").strip()\n",
    "    when_utc = _to_utc(when)\n",
    "    \n",
    "    def earliest_ancestor(b, ts):\n",
    "        if b in reverse_map and ts is not None:\n",
    "            candidates = [(src, eff) for (src, eff) in reverse_map[b] if ts >= eff]\n",
    "            if candidates:\n",
    "                src, eff = max(candidates, key=lambda x: x[1])\n",
    "                return earliest_ancestor(src, ts)\n",
    "        return b\n",
    "    \n",
    "    def chain_label(root):\n",
    "        visited = set([root])\n",
    "        segs = [root]\n",
    "        cur = root\n",
    "        while cur in forward_map:\n",
    "            nxt, _ = sorted(forward_map[cur], key=lambda x: x[1])[0]\n",
    "            if nxt in visited:\n",
    "                break\n",
    "            segs.append(nxt)\n",
    "            visited.add(nxt)\n",
    "            cur = nxt\n",
    "        return \"→\".join(segs)\n",
    "    \n",
    "    anc = earliest_ancestor(brand, when_utc)\n",
    "    label = chain_label(anc)\n",
    "    era = _era_for(brand, when_utc)\n",
    "    return f\"CHAIN:{label} | ERA:{era}\"\n",
    "\n",
    "series_roster[\"series_datetime\"] = pd.to_datetime(series_roster[\"series_datetime\"], utc=True, errors=\"coerce\")\n",
    "series_roster[\"continuity_key\"] = series_roster.apply(\n",
    "    lambda r: continuity_key_for(r[\"team_full\"], r[\"series_datetime\"]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Apply disambiguation to df_final - need to get roster info first\n",
    "# Get roster from player_stats merge\n",
    "roster_lookup = (\n",
    "    player_stats.rename(columns={'game_id': 'series_id'})\n",
    "    .groupby(['series_id', 'player_team_full'])['player_handle']\n",
    "    .apply(lambda lst: sorted(set(lst)))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Merge roster into df_final\n",
    "df_final = df_final.merge(\n",
    "    roster_lookup.rename(columns={'series_id': 'Map1_ID', 'player_team_full': 'team'}),\n",
    "    on=['Map1_ID', 'team'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Set team name columns\n",
    "df_final['team_full_raw'] = df_final['team']\n",
    "df_final['team_full_corr'] = df_final['team']\n",
    "\n",
    "df_final[\"series_datetime\"] = pd.to_datetime(df_final[\"series_datetime\"], utc=True, errors=\"coerce\")\n",
    "df_final[\"continuity_key\"] = df_final.apply(\n",
    "    lambda r: continuity_key_for(r[\"team_full_corr\"], r[\"series_datetime\"]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "team_series = {\n",
    "    key: grp.sort_values('series_datetime').reset_index(drop=True)\n",
    "    for key, grp in series_roster.groupby('continuity_key', dropna=False)\n",
    "}\n",
    "\n",
    "def compute_days_since_fast(roster, current_date, k, cont_key):\n",
    "    if not isinstance(roster, (list, tuple)) or pd.isna(current_date) or pd.isna(cont_key):\n",
    "        return pd.NA\n",
    "    if cont_key not in team_series:\n",
    "        return pd.NA\n",
    "    past = team_series[cont_key]\n",
    "    past = past[past['series_datetime'] <= current_date]\n",
    "    if past.empty:\n",
    "        return pd.NA\n",
    "    key = set(roster)\n",
    "    mask = past['player_handle'].apply(lambda ph: len(set(ph).intersection(key)) >= k)\n",
    "    if not mask.any():\n",
    "        return 0\n",
    "    first = past.loc[mask, 'series_datetime'].min()\n",
    "    return int((current_date - first).total_seconds() // 86400)\n",
    "\n",
    "for k in (3, 4, 5):\n",
    "    df_final[f'days_since_{k}_of_5'] = df_final.apply(\n",
    "        lambda r: compute_days_since_fast(\n",
    "            r.get('player_handle'),\n",
    "            r['series_datetime'],\n",
    "            k,\n",
    "            r['continuity_key']\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "df_final = df_final.sort_values(['continuity_key', 'series_datetime'])\n",
    "df_final['DaysSinceLastMatch'] = (\n",
    "    df_final.groupby('continuity_key')['series_datetime']\n",
    "    .diff()\n",
    "    .dt.total_seconds()\n",
    "    .div(86400)\n",
    "    .astype('float')\n",
    ")\n",
    "\n",
    "df_final = df_final.drop(columns=['player_handle'], errors='ignore').drop_duplicates()\n",
    "\n",
    "def parse_pb_cell(s):\n",
    "    if not isinstance(s, str) or not s.strip():\n",
    "        return None\n",
    "    toks = s.split()\n",
    "    if len(toks) >= 3 and toks[1].lower() in {'pick', 'ban'}:\n",
    "        return toks[2]\n",
    "    if 'remains' in s.lower():\n",
    "        return toks[0] if toks else None\n",
    "    if len(toks) == 1 and toks[0][0].isupper():\n",
    "        return toks[0]\n",
    "    return None\n",
    "\n",
    "for col in pb_cols:\n",
    "    if col in df_final.columns:\n",
    "        df_final[col] = df_final[col].apply(parse_pb_cell)\n",
    "\n",
    "df_final['pb_valid_count'] = df_final[pb_cols].apply(lambda row: sum(pd.notna(row)), axis=1)\n",
    "df_final = df_final[df_final['pb_valid_count'] > 0].drop(columns='pb_valid_count')\n",
    "\n",
    "df_final['num_real_picks'] = df_final[pb_cols].apply(\n",
    "    lambda row: sum(isinstance(val, str) and val.lower() != 'remains' for val in row),\n",
    "    axis=1\n",
    ")\n",
    "df_final = df_final[df_final['num_real_picks'] >= 2].drop(columns='num_real_picks')\n",
    "\n",
    "df_final['MapSet'] = df_final[pb_cols].values.tolist()\n",
    "df_final['MapSet'] = df_final['MapSet'].apply(lambda x: frozenset(filter(None, x)))\n",
    "\n",
    "print(\"Rows before 7-map gate:\", len(df_final))\n",
    "df_final = df_final[df_final['MapSet'].apply(len) == 7]\n",
    "print(\"Rows after 7-map gate:\", len(df_final))\n",
    "\n",
    "df_final = df_final.sort_values('series_datetime')\n",
    "unique_mapsets = df_final['MapSet'].drop_duplicates()\n",
    "total_pools = len(unique_mapsets)\n",
    "pool_id_map = {ms: (total_pools - i) for i, ms in enumerate(unique_mapsets)}\n",
    "\n",
    "df_final['PoolID'] = df_final['MapSet'].map(pool_id_map)\n",
    "df_final['ActiveMapPool'] = df_final['MapSet'].apply(lambda x: sorted(list(x)))\n",
    "df_final['ReversedPoolID'] = df_final['PoolID'].max() + 1 - df_final['PoolID']\n",
    "df_final = df_final.drop(columns=['MapSet'])\n",
    "\n",
    "print(f\"PoolID gaps check: {sorted(df_final['PoolID'].unique())}\")\n",
    "\n",
    "df_final = df_final.sort_values(['series_datetime', 'team1_name', 'team2_name'])\n",
    "match_id_remap = {old_id: new_id for new_id, old_id in enumerate(sorted(df_final['MatchID'].unique()), 1)}\n",
    "df_final['MatchID'] = df_final['MatchID'].map(match_id_remap)\n",
    "\n",
    "def clean_patch_string(patch):\n",
    "    s = str(patch).strip()\n",
    "    m = re.search(r'(\\d+)\\.(\\d+)', s)\n",
    "    return f\"Patch {m.group(1)}.{m.group(2)}\" if m else np.nan\n",
    "\n",
    "def patch_to_index(patch):\n",
    "    if pd.isna(patch):\n",
    "        return np.nan\n",
    "    s = str(patch).strip()\n",
    "    m = re.search(r'(\\d+)\\.(\\d+)', s)\n",
    "    if not m:\n",
    "        return np.nan\n",
    "    major, minor = int(m.group(1)), int(m.group(2))\n",
    "    return major * 100 + minor\n",
    "\n",
    "df_final['match_patch'] = df_final['match_patch'].apply(clean_patch_string)\n",
    "df_final['patch_index'] = df_final['match_patch'].apply(patch_to_index)\n",
    "df_final['patch_major'] = df_final['match_patch'].astype(str).str.extract(r'(\\d+)').astype('Int64')\n",
    "\n",
    "nan_patches = df_final['patch_index'].isna().sum()\n",
    "print(f\"NaN patch_index rows: {nan_patches}\")\n",
    "\n",
    "for col in ['team', 'team1_name', 'team2_name']:\n",
    "    if col in df_final.columns:\n",
    "        df_final = df_final[~df_final[col].str.contains(\"Spotlight\", na=False)]\n",
    "\n",
    "df_final = df_final[df_final['patch_index'].notna()]\n",
    "\n",
    "df_final = df_final[\n",
    "    df_final.get('team1_pick_1', pd.Series([pd.NA])).notna() &\n",
    "    df_final.get('team2_pick_1', pd.Series([pd.NA])).notna()\n",
    "]\n",
    "\n",
    "df_final = df_final[df_final['Map1_RD'].notna() & df_final['Map2_RD'].notna()]\n",
    "\n",
    "df_final.to_csv(OUT_PATH, index=False)\n",
    "print(f\"Final output: {len(df_final)} records saved → {OUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbdc6e6-56a7-4956-890b-349333796d5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (juicer)",
   "language": "python",
   "name": "juicer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
