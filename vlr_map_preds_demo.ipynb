{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e5a4739-f05e-4d31-884e-2817506de360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map preds saved: /Users/samharwood/Downloads/map_preds_demo.csv, rows=6230\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "DATA_PATH     = \"data/vlr_patchpool_demo.csv\"\n",
    "FEATURES_PATH = \"data/vlr_features_demo.csv\"\n",
    "OUTPUT_PATH   = \"data/vlr_map_preds_demo.csv\"\n",
    "MIN_MATCHES   = 5\n",
    "\n",
    "# Hyperparams (demo values; not tuned. Placeholder values also in place for picks/bans)\n",
    "PATCH_LAM  = 0.5 #patch decay\n",
    "REC_LAM    = 0.5 #time based recency decay\n",
    "CORE_LAM   = 0.5 #roster continuity decay\n",
    "MOV_LAM    = 0.5 #margin of victory weight\n",
    "MARGIN_W   = 0.5 #relative weight for MOV signal vs pick and ban signal\n",
    "EPSILON    = 0.5 #baseline probability floor\n",
    "\n",
    "#pick/ban weights; positive values increase map probability, negative values decrease it\n",
    "PB_WEIGHTS = {\n",
    "    \"pick_1\":  0.5, \"pick_2\": 0.5, \"pick_3\": 0.5,\n",
    "    \"ban_1\":  -0.5, \"ban_2\": -0.5, \"ban_3\": -0.5\n",
    "}\n",
    "\n",
    "def canonicalize_team(s: str) -> str:\n",
    "    s = str(s).strip()\n",
    "    return \"NRG\" if s.lower() == \"nrg esports\" else s\n",
    "\n",
    "#round ts made to conform to nearest 30m slot\n",
    "def canonical_match_time(ts: pd.Timestamp) -> pd.Timestamp:\n",
    "    ts = pd.Timestamp(ts)\n",
    "    m = ts.minute + ts.second/60 + ts.microsecond/6e7\n",
    "    if m < 15:  return ts.floor(\"h\").replace(minute=0,  second=0, microsecond=0)\n",
    "    if m < 45:  return ts.floor(\"h\").replace(minute=30, second=0, microsecond=0)\n",
    "    return (ts.floor(\"h\") + pd.Timedelta(hours=1)).replace(minute=0, second=0, microsecond=0)\n",
    "\n",
    "#parses bracketed map pool str into list\n",
    "def parse_active_pool_string(s):\n",
    "    if pd.isna(s): return []\n",
    "    ss = str(s).strip().strip(\"[]\").replace(\"'\", \"\")\n",
    "    parts = [p.strip().lower() for p in ss.split(\",\") if p.strip()]\n",
    "    return parts\n",
    "\n",
    "#returns expected maps to win based on series duration\n",
    "def pick_expected_maps(best_of_value):\n",
    "    try:\n",
    "        bo = int(best_of_value)\n",
    "        return 2 if bo == 3 else (3 if bo == 5 else 2)\n",
    "    except Exception:\n",
    "        return 2\n",
    "        \n",
    "# Stable softmax scaled to target_sum/series duration; NaN/overflow handling\n",
    "def softmax_to_target(x, target_sum):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if x.size == 0: return x\n",
    "    x = x - np.nanmax(x)\n",
    "    ex = np.exp(x)\n",
    "    s = ex.sum()\n",
    "    if not np.isfinite(s) or s <= 0:\n",
    "        return np.full_like(x, fill_value=(target_sum / max(len(x), 1)))\n",
    "    return ex / s * target_sum\n",
    "\n",
    "# Normalize probs so they sum to target_sum while enforcing an upper cap per entry.\n",
    "# Remaining probability mass is redistributed fairly to non-capped map probability entries\n",
    "def cap_and_waterfill(probs, target_sum, cap=1.0, tol=1e-9, max_iter=50):\n",
    "    p = np.clip(np.asarray(probs, dtype=float), 0.0, np.inf)\n",
    "    p = np.minimum(p, cap)\n",
    "    total = p.sum()\n",
    "    if target_sum <= 0: return np.zeros_like(p)\n",
    "    if abs(total - target_sum) <= tol: return p\n",
    "    if total > target_sum + tol: return p * (target_sum / total)\n",
    "    remaining = target_sum - total\n",
    "    mask = p < cap\n",
    "    it = 0\n",
    "    while remaining > tol and mask.any() and it < max_iter:\n",
    "        room = np.where(mask, cap - p, 0.0)\n",
    "        room_sum = room.sum()\n",
    "        if room_sum <= tol: break\n",
    "        add = remaining * (room / room_sum)\n",
    "        p = p + add\n",
    "        over = p > cap\n",
    "        if over.any(): p[over] = cap\n",
    "        remaining = target_sum - p.sum()\n",
    "        mask = p < cap - 1e-12\n",
    "        it += 1\n",
    "    return p\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "features = pd.read_csv(FEATURES_PATH, parse_dates=[\"series_datetime\"])\n",
    "\n",
    "\n",
    "df[\"team1_name\"] = df[\"team1_name\"].astype(str).str.strip()\n",
    "df[\"team2_name\"] = df[\"team2_name\"].astype(str).str.strip()\n",
    "df[\"series_datetime\"] = pd.to_datetime(df[\"series_datetime\"], errors=\"coerce\")\n",
    "df[\"team1_name_canon\"] = df[\"team1_name\"].apply(canonicalize_team)\n",
    "df[\"team2_name_canon\"] = df[\"team2_name\"].apply(canonicalize_team)\n",
    "\n",
    "#lowercase parsing of active pool\n",
    "df[\"match_active_pool\"] = df[\"ActiveMapPool\"].apply(parse_active_pool_string)\n",
    "\n",
    "#fill value for roster continuity; if not present use large sample value\n",
    "for k in (3, 4, 5):\n",
    "    col = f\"days_since_{k}_of_5\"\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\").fillna(999).astype(int)\n",
    "    else:\n",
    "        df[col] = 999\n",
    "\n",
    "#remove fake teams\n",
    "mask = (df[\"team1_name\"].str.contains(r\"\\(.*\\)\", regex=True, na=False)\n",
    "        | df[\"team2_name\"].str.contains(r\"\\(.*\\)\", regex=True, na=False))\n",
    "df = df[~mask].reset_index(drop=True)\n",
    "\n",
    "#assign MatchID if missing\n",
    "df[\"MatchID\"] = (df[\"MatchID\"].astype(str) if \"MatchID\" in df.columns else df.index.astype(str))\n",
    "\n",
    "#unique series_datetime, patch_index pairs sorted chronologically\n",
    "patch_timeline = (features[[\"series_datetime\", \"patch_index\"]]\n",
    "                  .dropna().drop_duplicates().sort_values(\"series_datetime\"))\n",
    "\n",
    "#as-of lookup that returns most recent patch_index active at timestamp ts\n",
    "def patch_index_asof(ts):\n",
    "    if pd.isna(ts) or len(patch_timeline) == 0: return 0.0\n",
    "    idx = patch_timeline[\"series_datetime\"].searchsorted(pd.Timestamp(ts), side=\"left\") - 1\n",
    "    if idx >= 0: return float(patch_timeline.iloc[idx][\"patch_index\"])\n",
    "    return float(patch_timeline.iloc[0][\"patch_index\"])\n",
    "\n",
    "#active pool timeline from patchpool\n",
    "pp_pools = (df[[\"series_datetime\", \"ActiveMapPool\"]]\n",
    "            .dropna().drop_duplicates().sort_values(\"series_datetime\"))\n",
    "\n",
    "#returns active map pool as of timestamp ts; finds latest pool entry before ts and falls back to prior map list if current isn't complete (7 maps)\n",
    "def active_pool_asof(ts):\n",
    "    if len(pp_pools) == 0: return []\n",
    "    idx = pp_pools[\"series_datetime\"].searchsorted(pd.Timestamp(ts), side=\"left\") - 1\n",
    "    if idx < 0: idx = 0\n",
    "    pool = parse_active_pool_string(pp_pools.iloc[idx][\"ActiveMapPool\"])\n",
    "    if len(pool) < 7 and idx > 0:\n",
    "        for j in range(idx, -1, -1):\n",
    "            alt = parse_active_pool_string(pp_pools.iloc[j][\"ActiveMapPool\"])\n",
    "            if len(alt) >= 7: return list(dict.fromkeys(alt))\n",
    "    return list(dict.fromkeys(pool))\n",
    "\n",
    "#builds per-team match history dict; for each canonical team, collect and sort all matches chronologically\n",
    "team_histories = {}\n",
    "all_teams_canon = pd.unique(pd.concat([df[\"team1_name_canon\"], df[\"team2_name_canon\"]]))\n",
    "for team in all_teams_canon:\n",
    "    hist = df[(df[\"team1_name_canon\"] == team) | (df[\"team2_name_canon\"] == team)].copy()\n",
    "    hist = hist.sort_values(\"series_datetime\").reset_index(drop=True)\n",
    "    team_histories[team] = hist\n",
    "\n",
    "#extract and clean core feature fields\n",
    "feat = features[['game_id', 'series_datetime', 'player_team_full', 'player_handle']].dropna()\n",
    "feat['game_id'] = pd.to_numeric(feat['game_id'], errors='coerce')\n",
    "feat = feat.dropna(subset=['game_id'])\n",
    "feat['player_team_full'] = feat['player_team_full'].astype(str).str.strip()\n",
    "feat['player_handle'] = feat['player_handle'].astype(str).str.strip()\n",
    "\n",
    "#per game_id, team roster sets built\n",
    "feat_rosters = (\n",
    "    feat.groupby(['player_team_full', 'game_id'])['player_handle']\n",
    "        .apply(lambda s: frozenset(set(s)))\n",
    "        .reset_index()\n",
    ")\n",
    "\n",
    "# Attach per-game series_datetime for ordering\n",
    "gid_time = (features[['game_id', 'series_datetime']]\n",
    "            .dropna()\n",
    "            .drop_duplicates()\n",
    "            .assign(series_datetime=lambda d: pd.to_datetime(d['series_datetime'], errors='coerce')))\n",
    "\n",
    "#merge maths timestamps, drop unmatched rosters, sort rosters chronologically at team level\n",
    "feat_rosters = feat_rosters.merge(gid_time, on='game_id', how='left')\n",
    "feat_rosters = feat_rosters.dropna(subset=['series_datetime'])\n",
    "feat_rosters = feat_rosters.sort_values(['player_team_full', 'series_datetime'])\n",
    "\n",
    "#detect full roster resets (no shared players with prior lineup), record reset timestamps per team\n",
    "team_reset_times = {}  # keys will include both original label and canonicalized label for robustness\n",
    "for team_label, g in feat_rosters.groupby('player_team_full'):\n",
    "    g = g.sort_values('series_datetime').reset_index(drop=True)\n",
    "    resets = []\n",
    "    prev_r = None\n",
    "    for _, row in g.iterrows():\n",
    "        rset = row['player_handle']\n",
    "        t = row['series_datetime']\n",
    "        if prev_r is not None:\n",
    "            if len(prev_r.intersection(rset)) == 0:\n",
    "                resets.append(pd.Timestamp(t))\n",
    "        prev_r = rset\n",
    "    team_reset_times[team_label] = resets\n",
    "    canon = canonicalize_team(team_label)\n",
    "    if canon not in team_reset_times:\n",
    "        team_reset_times[canon] = resets\n",
    "        \n",
    "#return latest full-roster reset at team level that occurred before timestamp ts\n",
    "def last_reset_before(team_label: str, ts: pd.Timestamp):\n",
    "    if team_label not in team_reset_times or not team_reset_times[team_label]:\n",
    "        return None\n",
    "    times = [rt for rt in team_reset_times[team_label] if pd.Timestamp(rt) < pd.Timestamp(ts)]\n",
    "    return max(times) if times else None\n",
    "\n",
    "def compute_team_map_scores(hist, team_canon, patch_asof, sched_ts, pool_lc):\n",
    "    #initialize accumulators for picks/bans signal, round diff MOV signal\n",
    "    pb, mv = defaultdict(float), defaultdict(float)\n",
    "    team_rows = hist[(hist[\"team1_name_canon\"] == team_canon) | (hist[\"team2_name_canon\"] == team_canon)]\n",
    "    if team_rows.empty: return {m: EPSILON for m in pool_lc}\n",
    "    #check if map name/map rd exist for MOV based contribution\n",
    "    has_map_cols = all([(f\"Map{i}_name\" in hist.columns and f\"Map{i}_RD\" in hist.columns) for i in range(1,6)])\n",
    "\n",
    "    #filter team rows on either side from history\n",
    "    for _, r in team_rows.iterrows():\n",
    "        if pd.to_numeric(r.get(\"days_since_3_of_5\", 999), errors=\"coerce\") == 0:\n",
    "            continue\n",
    "        #patch weight computation; further in the past/away from current patch means lower weight\n",
    "        prev_patch = float(pd.to_numeric(r.get(\"patch_index\", np.nan), errors=\"coerce\"))\n",
    "        if not np.isfinite(prev_patch):\n",
    "            prev_patch = patch_index_asof(r[\"series_datetime\"])\n",
    "        dp      = max(0.0, float(patch_asof) - prev_patch)\n",
    "        w_patch = 1.0 / (1.0 + PATCH_LAM * np.log1p(dp))\n",
    "\n",
    "        #time decay weight computation\n",
    "        dt_days = (pd.Timestamp(sched_ts) - r[\"series_datetime\"]).total_seconds() / 86400.0 if pd.notna(r[\"series_datetime\"]) else 999.0\n",
    "        w_time  = float(np.exp(-REC_LAM * max(0.0, dt_days)))\n",
    "\n",
    "        #roster continuity confidence calculated\n",
    "        d3 = pd.to_numeric(r.get(\"days_since_3_of_5\", 999), errors=\"coerce\"); d3 = 999.0 if pd.isna(d3) else float(d3)\n",
    "        d4 = pd.to_numeric(r.get(\"days_since_4_of_5\", 999), errors=\"coerce\"); d4 = 999.0 if pd.isna(d4) else float(d4)\n",
    "        d5 = pd.to_numeric(r.get(\"days_since_5_of_5\", 999), errors=\"coerce\"); d5 = 999.0 if pd.isna(d5) else float(d5)\n",
    "        conf = 0.2*(1-np.exp(-CORE_LAM*d3)) + 0.3*(1-np.exp(-CORE_LAM*d4)) + 0.5*(1-np.exp(-CORE_LAM*d5))\n",
    "\n",
    "        w = w_patch * w_time * conf\n",
    "        if w <= 0: continue\n",
    "\n",
    "        #determine which side team was on for PB column prefixes\n",
    "        if r.get(\"team1_name_canon\") == team_canon:\n",
    "            prefixes = (\"team1_\",)\n",
    "        elif r.get(\"team2_name_canon\") == team_canon:\n",
    "            prefixes = (\"team2_\",)\n",
    "        else:\n",
    "            prefixes = ()\n",
    "\n",
    "        #accumulate pick/ban signal for maps in the current pool\n",
    "        for base, wt in PB_WEIGHTS.items():\n",
    "            for pref in prefixes:\n",
    "                m = r.get(pref + base)\n",
    "                if isinstance(m, str):\n",
    "                    mm = m.strip().lower()\n",
    "                    if mm in pool_lc:\n",
    "                        pb[mm] += wt * w\n",
    "                        \n",
    "        #accumulate MOV signal at map level\n",
    "        if has_map_cols:\n",
    "            for i in range(1, 6):\n",
    "                m  = r.get(f\"Map{i}_name\")\n",
    "                rd = r.get(f\"Map{i}_RD\")\n",
    "                if isinstance(m, str) and pd.notna(rd):\n",
    "                    mm = m.strip().lower()\n",
    "                    if mm in pool_lc:\n",
    "                        mv[mm] += float(rd) * w * MOV_LAM\n",
    "                        \n",
    "    #combine PB, MOV, weight signals with epsilon\n",
    "    out = {}\n",
    "    for m in pool_lc:\n",
    "        out[m] = pb.get(m, 0.0) + mv.get(m, 0.0) * MARGIN_W + EPSILON\n",
    "    return out\n",
    "\n",
    "#build per-match context: get canonical start time, 2h prematch cutoff, canonical team names\n",
    "contexts = []\n",
    "df_matches = df.drop_duplicates(subset=\"MatchID\")\n",
    "for _, row in df_matches.iterrows():\n",
    "    raw_ts = pd.Timestamp(row[\"series_datetime\"])\n",
    "    sched  = canonical_match_time(raw_ts)\n",
    "    cutoff = sched - pd.Timedelta(hours=2)\n",
    "\n",
    "    t1_c = row[\"team1_name_canon\"]; t2_c = row[\"team2_name_canon\"]\n",
    "\n",
    "#reset-aware history window: restrict each teamâ€™s past matches to its current roster era before cutoff\n",
    "    lr1 = last_reset_before(t1_c, cutoff)\n",
    "    if lr1 is None:\n",
    "        lr1 = last_reset_before(str(row.get(\"team1_name\", \"\")), cutoff)\n",
    "    lr2 = last_reset_before(t2_c, cutoff)\n",
    "    if lr2 is None:\n",
    "        lr2 = last_reset_before(str(row.get(\"team2_name\", \"\")), cutoff)\n",
    "\n",
    "    base_h1 = team_histories[t1_c][team_histories[t1_c][\"series_datetime\"] < cutoff]\n",
    "    base_h2 = team_histories[t2_c][team_histories[t2_c][\"series_datetime\"] < cutoff]\n",
    "\n",
    "    h1 = base_h1[base_h1[\"series_datetime\"] >= lr1] if lr1 is not None else base_h1\n",
    "    h2 = base_h2[base_h2[\"series_datetime\"] >= lr2] if lr2 is not None else base_h2\n",
    "\n",
    "    if len(h1) < MIN_MATCHES or len(h2) < MIN_MATCHES:\n",
    "        continue\n",
    "\n",
    "    patch_asof = patch_index_asof(sched)\n",
    "    pool = active_pool_asof(sched)\n",
    "    if not isinstance(pool, list) or len(pool) < 7:\n",
    "        continue\n",
    "    # lowercased pool, stable order\n",
    "    pool = list(dict.fromkeys([p.strip().lower() for p in pool if isinstance(p, str) and p.strip()]))\n",
    "\n",
    "    #expected maps by best_of; keep played maps only for evaluation\n",
    "    best_of = row.get(\"match_best_of\", 3)\n",
    "    target_map_count = pick_expected_maps(best_of)\n",
    "\n",
    "    #extract valid player maps from this match; limit to target_map_count, filter to current pool\n",
    "    played_maps = []\n",
    "    for i in range(1, 6):\n",
    "        m = row.get(f\"Map{i}_name\")\n",
    "        if isinstance(m, str):\n",
    "            mm = m.strip().lower()\n",
    "            if mm in pool:\n",
    "                played_maps.append(mm)\n",
    "        if len(played_maps) >= target_map_count:\n",
    "            break\n",
    "    #skip matches with incomplete, invalid map data\n",
    "    if len(played_maps) != target_map_count:\n",
    "        continue\n",
    "    #store as set for quick membership checks\n",
    "    played_set = set(played_maps)\n",
    "\n",
    "    contexts.append((h1, h2, pool, sched, patch_asof, played_set, t1_c, t2_c, row[\"MatchID\"], raw_ts, target_map_count))\n",
    "\n",
    "#compute per-match combined maps scores for both teams\n",
    "csv_rows = []\n",
    "for h1, h2, pool, sched, patch_asof, played_set, t1_c, t2_c, match_id, raw_ts, target_sum in contexts:\n",
    "    #compute individual team map scores weighted by patch/time/roster features\n",
    "    s1 = compute_team_map_scores(h1, t1_c, patch_asof, sched, pool)\n",
    "    s2 = compute_team_map_scores(h2, t2_c, patch_asof, sched, pool)\n",
    "    #combine both teams' scores per map into joint raw score vector \n",
    "    raw_scores = np.array([s1[m] + s2[m] for m in pool], dtype=float)\n",
    "\n",
    "    base = softmax_to_target(raw_scores, target_sum)\n",
    "    final_probs = cap_and_waterfill(base, target_sum, cap=1.0)\n",
    "\n",
    "    for m, p in zip(pool, final_probs):\n",
    "        csv_rows.append({\n",
    "            \"MatchID\": match_id,\n",
    "            \"team1_name\": t1_c,\n",
    "            \"team2_name\": t2_c,\n",
    "            \"series_datetime_provider\": raw_ts,         # for audit\n",
    "            \"series_datetime_canonical\": sched,         # used for computation\n",
    "            \"map_name\": m,                              # lowercased\n",
    "            \"map_predicted_prob\": float(p),\n",
    "            \"expected_maps\": target_sum,\n",
    "            \"map_was_played\": 1.0 if m in played_set else 0.0,\n",
    "            \"patch_index_asof\": float(patch_asof),\n",
    "            \"patch_index\": float(patch_asof),           # kept for backward-compat\n",
    "        })\n",
    "\n",
    "df_out = pd.DataFrame(csv_rows)\n",
    "df_out = df_out.sort_values([\"series_datetime_canonical\",\"MatchID\",\"map_name\"]).reset_index(drop=True)\n",
    "\n",
    "#sanity check for no duplicate matchid, map_name combos\n",
    "assert not df_out.duplicated(subset=[\"MatchID\",\"map_name\"]).any(), \"Duplicate (MatchID, map_name)!\"\n",
    "\n",
    "#verifies cap logic, ensuring no map probability exceeds 1.0\n",
    "viol = (df_out[\"map_predicted_prob\"] > 1.0 + 1e-9).sum()\n",
    "if viol:\n",
    "    raise AssertionError(f\"Found {viol} rows with map_predicted_prob > 1.0 (cap failed).\")\n",
    "\n",
    "#check softmax normalization; per-match map probabilities sum= expected maps\n",
    "sums = (df_out.groupby([\"MatchID\",\"expected_maps\"])\n",
    "              .agg(pred_sum=(\"map_predicted_prob\",\"sum\")).reset_index())\n",
    "drift = (sums[\"pred_sum\"] - sums[\"expected_maps\"]).abs().max() if len(sums) else 0.0\n",
    "if drift and drift > 1e-6:\n",
    "    print(f\"Note: tiny mass drift observed (max {drift:.6f}); acceptable.\")\n",
    "\n",
    "df_out.to_csv(OUTPUT_PATH, index=False)\n",
    "print(f\"Map preds saved: {OUTPUT_PATH}, rows={len(df_out)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5f81dc-257b-43d0-b1d1-7fbfbef1a699",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (juicer)",
   "language": "python",
   "name": "juicer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
